{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Data Lake to Data Warehouse\n",
    "\n",
    "### What the Notebook Does\n",
    "\n",
    "- **Data Loading and Parsing:**  \n",
    "  The notebook reads HTML files containing newspaper articles along with their corresponding metadata from a CSV file. It then uses BeautifulSoup to parse the HTML and extract only the relevant content needed for further analysis.\n",
    "\n",
    "- **Data Processing and Preparation:**  \n",
    "  The extracted content is processed to isolate the contexts in which the term \"klima\" appears. This includes capturing the surrounding text to better understand the usage and meaning of the word in each article.\n",
    "\n",
    "- **Data Storage:**  \n",
    "  The processed data is structured and stored in a SQLite database with two tables. This ensures that the data is organized, easily accessible, and ready for further analysis. It will also export the data as csv for an easy import to other programms.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "- **Table: newspaper**  \n",
    "  Stores metadata about each newspaper's main page, including the publication details corresponding to a single day.  \n",
    "  Each entry represents the main page of a newspaper for one day, as the dataset is derived from crawling the main page rather than individual articles. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `newspaper_name`  \n",
    "  - `data_published`  \n",
    "  - `klima_mentions_count`\n",
    "\n",
    "- **Table: context**  \n",
    "  Contains detailed text snippets surrounding the target word \"klima\". The id refers to a newspaper (main page) from one specific day. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `pre_context`  \n",
    "  - `post_context`  \n",
    "  - `prefix`  \n",
    "  - `suffix`\n",
    "\n",
    "### Why This Approach\n",
    "\n",
    "- **Focused Analysis:**  \n",
    "  By isolating the contexts where \"klima\" is mentioned, the notebook prepares data specifically tailored to analyze the evolution of the term's usage over time.\n",
    "\n",
    "- **Data Organization:**  \n",
    "  Storing data in a structured SQLite database facilitates efficient querying and analysis, ensuring that subsequent analytical processes can be performed seamlessly.\n",
    "\n",
    "- **Reproducibility and Scalability:**  \n",
    "  This clear separation of tasks—from data extraction to storage—supports a reproducible workflow that can easily be extended or modified for future analytical targets.\n",
    "\n",
    "For additional details and background, please refer to the README file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "import pandas as pd\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "from handle_data_processing import batch_process_newspapers\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All the Newspapers\n",
    "\n",
    "In this section, we load the CSV files that contain details for each newspaper, such as file path, date, and HTTP status code. Each file represents data from one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to list all CSV files in the specified directory that follow a date format in their names\n",
    "\n",
    "csv_dir = os.path.join(notebook_dir, \"..\", \"data_input\", \"data-lake\")\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*-*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the files based on the date portion of the filename (ignoring the directory path). This help in the long taking processing step to easily track progress from the start date to the end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of total days: 8\n"
     ]
    }
   ],
   "source": [
    "# we sort by the filename which contains the date, ignoring the directory path to make the sort efficient\n",
    "csv_files.sort(key=lambda f: f.split('/')[-1])\n",
    "\n",
    "# Output the total count of days (CSV files) to confirm the number of entries\n",
    "print(f'Count of total days: {len(csv_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read the CSV files one by one to retrieve the HTML file paths, including only those with a status code of 200 (OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store newspaper data\n",
    "newspapers = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        # For each row, check if the status code is 200 (OK) and append the newspaper data to the list\n",
    "        for row in reader:\n",
    "            if row['status'] == '200':\n",
    "                newspapers.append({\n",
    "                    'name': row['name'],\n",
    "                    'date': row['date'],\n",
    "                    'file_name': row['file_name'],\n",
    "                    'encoding': row['encoding']\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification: Display the first two newspaper entries to inspect the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sz',\n",
       "  'date': '2025-02-01',\n",
       "  'file_name': 'data-lake/2025-02-01-sz.html',\n",
       "  'encoding': 'utf-8'},\n",
       " {'name': 'zeit',\n",
       "  'date': '2025-02-01',\n",
       "  'file_name': 'data-lake/2025-02-01-zeit.html',\n",
       "  'encoding': 'UTF-8'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first two entries to verify the structure of the newspaper data\n",
    "newspapers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Actual Processing with Batch Processing and Multiprocessing\n",
    "\n",
    "Here we process the newspaper data in batches using multiprocessing. This step prepares the data by extracting the relevant HTML content and storing the results in a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:32:37,887 - INFO - Processing at date 2025-02-08.\n",
      "2025-09-12 22:32:37,897 - INFO - Data saved to table 'newspapers' in '/Users/edwinw/Documents/DBU Uni/wortwandel/notebooks/../data_output/dwh_data.db' successfully.\n",
      "2025-09-12 22:32:37,901 - INFO - Data saved to table 'context' in '/Users/edwinw/Documents/DBU Uni/wortwandel/notebooks/../data_output/dwh_data.db' successfully.\n"
     ]
    }
   ],
   "source": [
    "batch_process_newspapers(\n",
    "    newspapers,\n",
    "    batch_size=512,\n",
    "    num_workers=12,\n",
    "    db_path=os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\"),\n",
    "    input_path_prefix=os.path.join(notebook_dir, \"..\", \"data_input\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Saved Data\n",
    "\n",
    "After processing, we load the saved data from the SQLite database to verify that the data has been stored correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:32:38,234 - INFO - Data read from table 'newspapers' in '/Users/edwinw/Documents/DBU Uni/wortwandel/notebooks/../data_output_stick/dwh_data.db' successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "newspaper_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "data_published",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "klima_mentions_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "newspaper_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8db3eab2-7842-4795-a155-27bab3b068a3",
       "rows": [
        [
         "0",
         "sz",
         "2021-04-01",
         "6",
         "1"
        ],
        [
         "1",
         "zeit",
         "2021-04-01",
         "4",
         "2"
        ],
        [
         "2",
         "faz",
         "2021-04-01",
         "11",
         "3"
        ],
        [
         "3",
         "heise",
         "2021-04-01",
         "4",
         "4"
        ],
        [
         "4",
         "golem",
         "2021-04-01",
         "0",
         "5"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newspaper_name</th>\n",
       "      <th>data_published</th>\n",
       "      <th>klima_mentions_count</th>\n",
       "      <th>newspaper_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sz</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zeit</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faz</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heise</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>golem</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  newspaper_name data_published  klima_mentions_count  newspaper_id\n",
       "0             sz     2021-04-01                     6             1\n",
       "1           zeit     2021-04-01                     4             2\n",
       "2            faz     2021-04-01                    11             3\n",
       "3          heise     2021-04-01                     4             4\n",
       "4          golem     2021-04-01                     0             5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the 'newspapers' table from the database and display the first few rows\n",
    "meta_data = read_table_as_dataframe(\"newspapers\", db_path)\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:32:38,476 - INFO - Data read from table 'context' in '/Users/edwinw/Documents/DBU Uni/wortwandel/notebooks/../data_output_stick/dwh_data.db' successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pre_context",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "post_context",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prefix",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "suffix",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "newspaper_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b1272fd6-06ce-4a27-852d-badb541a9f14",
       "rows": [
        [
         "0",
         "im Zuge des",
         "werden dürfte. Kommentar",
         "",
         "wandels",
         "1"
        ],
        [
         "1",
         "Digital Alles zur",
         "Das Wichtigste auf",
         "",
         "krise",
         "1"
        ],
        [
         "2",
         "einer Seite Der",
         "Streitgespräch Wirtschaftswachstum und",
         "SZ",
         "monitor",
         "1"
        ],
        [
         "3",
         "Streitgespräch Wirtschaftswachstum und",
         "04:00 Video Warum",
         "",
         "krise",
         "1"
        ],
        [
         "4",
         "Golfstrom für das",
         "so wichtig ist",
         "",
         "",
         "1"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_context</th>\n",
       "      <th>post_context</th>\n",
       "      <th>prefix</th>\n",
       "      <th>suffix</th>\n",
       "      <th>newspaper_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im Zuge des</td>\n",
       "      <td>werden dürfte. Kommentar</td>\n",
       "      <td></td>\n",
       "      <td>wandels</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Alles zur</td>\n",
       "      <td>Das Wichtigste auf</td>\n",
       "      <td></td>\n",
       "      <td>krise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>einer Seite Der</td>\n",
       "      <td>Streitgespräch Wirtschaftswachstum und</td>\n",
       "      <td>SZ</td>\n",
       "      <td>monitor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Streitgespräch Wirtschaftswachstum und</td>\n",
       "      <td>04:00 Video Warum</td>\n",
       "      <td></td>\n",
       "      <td>krise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golfstrom für das</td>\n",
       "      <td>so wichtig ist</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pre_context  \\\n",
       "0                             im Zuge des   \n",
       "1                       Digital Alles zur   \n",
       "2                         einer Seite Der   \n",
       "3  Streitgespräch Wirtschaftswachstum und   \n",
       "4                       Golfstrom für das   \n",
       "\n",
       "                             post_context prefix   suffix  newspaper_id  \n",
       "0                werden dürfte. Kommentar         wandels             1  \n",
       "1                      Das Wichtigste auf           krise             1  \n",
       "2  Streitgespräch Wirtschaftswachstum und     SZ  monitor             1  \n",
       "3                       04:00 Video Warum           krise             1  \n",
       "4                          so wichtig ist                             1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the 'context' table from the database and display the first few rows\n",
    "context_data = read_table_as_dataframe(\"context\", db_path)\n",
    "context_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Processed Data as CSV Files (optional)\n",
    "\n",
    "Finally, we export the stored data as CSV files to facilitate further analysis in other programs that can't import sqlite files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the metadata and context data as CSV files, embedding today's date in the filenames\n",
    "meta_data.to_csv(\"dwh_meta_{today}.csv\", index=False)\n",
    "context_data.to_csv(\"dwh_context_{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique newspaper names in the metadata (64)\n",
    "# This helps verify that each newspaper is uniquely represented\n",
    "meta_data.newspaper_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique publication dates in the metadata (must equal 'Count of total days': 1401)\n",
    "# This ensures that we have distinct entries for each day the main page was crawled\n",
    "meta_data.data_published.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
