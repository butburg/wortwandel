{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Data Lake to Data Warehouse\n",
    "\n",
    "### What the Notebook Does\n",
    "\n",
    "- **Data Loading and Parsing:**  \n",
    "  The notebook reads HTML files containing newspaper articles along with their corresponding metadata from a CSV file. It then uses BeautifulSoup to parse the HTML and extract only the relevant content needed for further analysis.\n",
    "\n",
    "- **Data Processing and Preparation:**  \n",
    "  The extracted content is processed to isolate the contexts in which the term \"klima\" appears. This includes capturing the surrounding text to better understand the usage and meaning of the word in each article.\n",
    "\n",
    "- **Data Storage:**  \n",
    "  The processed data is structured and stored in a SQLite database with two tables. This ensures that the data is organized, easily accessible, and ready for further analysis. It will also export the data as csv for an easy import to other programms.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "- **Table: newspaper**  \n",
    "  Stores metadata about each newspaper's main page, including the publication details corresponding to a single day.  \n",
    "  Each entry represents the main page of a newspaper for one day, as the dataset is derived from crawling the main page rather than individual articles. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `newspaper_name`  \n",
    "  - `data_published`  \n",
    "  - `klima_mentions_count`\n",
    "\n",
    "- **Table: context**  \n",
    "  Contains detailed text snippets surrounding the target word \"klima\". The id refers to a newspaper (main page) from one specific day. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `pre_context`  \n",
    "  - `post_context`  \n",
    "  - `prefix`  \n",
    "  - `suffix`\n",
    "\n",
    "### Why This Approach\n",
    "\n",
    "- **Focused Analysis:**  \n",
    "  By isolating the contexts where \"klima\" is mentioned, the notebook prepares data specifically tailored to analyze the evolution of the term's usage over time.\n",
    "\n",
    "- **Data Organization:**  \n",
    "  Storing data in a structured SQLite database facilitates efficient querying and analysis, ensuring that subsequent analytical processes can be performed seamlessly.\n",
    "\n",
    "- **Reproducibility and Scalability:**  \n",
    "  This clear separation of tasks—from data extraction to storage—supports a reproducible workflow that can easily be extended or modified for future analytical targets.\n",
    "\n",
    "For additional details and background, please refer to the README file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "import pandas as pd\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "from handle_data_processing import batch_process_newspapers\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All the Newspapers\n",
    "\n",
    "In this section, we load the CSV files that contain details for each newspaper, such as file path, date, and HTTP status code. Each file represents data from one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to list all CSV files in the specified directory that follow a date format in their names\n",
    "\n",
    "csv_dir = os.path.join(notebook_dir, \"..\", \"data_input\", \"data-lake\")\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*-*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the files based on the date portion of the filename (ignoring the directory path). This help in the long taking processing step to easily track progress from the start date to the end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sort by the filename which contains the date, ignoring the directory path to make the sort efficient\n",
    "csv_files.sort(key=lambda f: f.split('/')[-1])\n",
    "\n",
    "# Output the total count of days (CSV files) to confirm the number of entries\n",
    "print(f'Count of total days: {len(csv_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read the CSV files one by one to retrieve the HTML file paths, including only those with a status code of 200 (OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store newspaper data\n",
    "newspapers = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        # For each row, check if the status code is 200 (OK) and append the newspaper data to the list\n",
    "        for row in reader:\n",
    "            if row['status'] == '200':\n",
    "                newspapers.append({\n",
    "                    'name': row['name'],\n",
    "                    'date': row['date'],\n",
    "                    'file_name': row['file_name'],\n",
    "                    'encoding': row['encoding']\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification: Display the first two newspaper entries to inspect the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first two entries to verify the structure of the newspaper data\n",
    "newspapers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Actual Processing with Batch Processing and Multiprocessing\n",
    "\n",
    "Here we process the newspaper data in batches using multiprocessing. This step prepares the data by extracting the relevant HTML content and storing the results in a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process_newspapers(\n",
    "    newspapers,\n",
    "    batch_size=512,\n",
    "    num_workers=12,\n",
    "    db_path=os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\"),\n",
    "    input_path_prefix=os.path.join(notebook_dir, \"..\", \"data_input\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Saved Data\n",
    "\n",
    "After processing, we load the saved data from the SQLite database to verify that the data has been stored correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the 'newspapers' table from the database and display the first few rows\n",
    "meta_data = read_table_as_dataframe(\"newspapers\", db_path)\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'context' table from the database and display the first few rows\n",
    "context_data = read_table_as_dataframe(\"context\", db_path)\n",
    "context_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Processed Data as CSV Files (optional)\n",
    "\n",
    "Finally, we export the stored data as CSV files to facilitate further analysis in other programs that can't import sqlite files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the metadata and context data as CSV files, embedding today's date in the filenames\n",
    "meta_data.to_csv(\"dwh_meta_{today}.csv\", index=False)\n",
    "context_data.to_csv(\"dwh_context_{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique newspaper names in the metadata (64)\n",
    "# This helps verify that each newspaper is uniquely represented\n",
    "meta_data.newspaper_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique publication dates in the metadata (must equal 'Count of total days': 1401)\n",
    "# This ensures that we have distinct entries for each day the main page was crawled\n",
    "meta_data.data_published.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
