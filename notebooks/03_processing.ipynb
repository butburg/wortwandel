{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing: Lemmatization and Lowercasing\n",
    "\n",
    "### What the Notebook Does\n",
    "\n",
    "- **Load Context Data:**  \n",
    "  Read the `context` table from the DWH database and focus on the text columns we want to normalize.\n",
    "\n",
    "- **Normalize Text:**  \n",
    "  Convert `pre_context`, `post_context`, `prefix`, and `suffix` to lowercase for consistent analysis.\n",
    "\n",
    "- **Manual Suffix Lemmatization:**  \n",
    "  Apply a curated mapping (created from a manual check) to normalize suffix variants into a common lemma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if \"__file__\" in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "import pandas as pd\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Context Data\n",
    "\n",
    "We load the `context` table and focus on the text columns we want to normalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = read_table_as_dataframe(\"context\", db_path)\n",
    "\n",
    "text_columns = [\"pre_context\", \"post_context\", \"prefix\", \"suffix\"]\n",
    "context[text_columns] = context[text_columns].astype(\"string\")\n",
    "context.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase All Text Columns\n",
    "\n",
    "This ensures consistent casing before any manual or automated normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in text_columns:\n",
    "    context[column] = context[column].str.lower()\n",
    "\n",
    "context[text_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness and Class Shares (Before Lemmatization)\n",
    "\n",
    "We summarize uniqueness and relative shares of suffix classes before lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_stats_before = (\n",
    "    context['suffix']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('suffix')\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "suffix_stats_before['share'] = suffix_stats_before['count'] / suffix_stats_before['count'].sum()\n",
    "\n",
    "suffix_stats_before.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_uniqueness_before = pd.DataFrame({\n",
    "    'total_rows': [len(context)],\n",
    "    'unique_suffixes': [context['suffix'].nunique(dropna=False)],\n",
    "})\n",
    "\n",
    "suffix_uniqueness_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lemma Candidates from Prefixes and Suffixes\n",
    "\n",
    "We generate candidate lemma groups using conservative German suffix rules.\n",
    "Short bases only allow small plural-style endings (e.g., +s, +n, +en),\n",
    "while longer bases allow limited extra length (about 25%).\n",
    "Overlapping groups are merged so a key appearing in another group pulls the sets together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Algorithm overview:\n",
    "# 1) Build candidate groups by matching words that differ only by small German suffixes\n",
    "#    (very short bases: only +s; 4-letter bases: +s/+e/+n/+en; longer bases: up to ~25% extra length).\n",
    "# 2) Pick the shortest word as the lemma key for each group.\n",
    "# 3) Merge overlapping groups so that if a variant is also a key, the groups are unified.\n",
    "# This keeps pairs like 'fest'/'festen' and avoids over-broad stems like 'etf' -> 'ethik'.\n",
    "\n",
    "def build_lemma_candidates_from_words(words, min_len=3):\n",
    "    words = pd.Series(words, dtype='string')\n",
    "    words = words.dropna().str.strip().str.lower()\n",
    "    words = words[words.str.len() >= min_len]\n",
    "\n",
    "    unique_words = sorted(set(words.tolist()), key=lambda w: (len(w), w))\n",
    "    assigned = set()\n",
    "    candidates = {}\n",
    "\n",
    "    def max_extra_len(base_len):\n",
    "        if base_len <= 3:\n",
    "            return 1\n",
    "        if base_len == 4:\n",
    "            return 2\n",
    "        return max(1, math.ceil(base_len * 0.25))\n",
    "\n",
    "    def allowed_small_suffix(extra, base_len):\n",
    "        if base_len <= 3:\n",
    "            return extra == 's'\n",
    "        if base_len == 4:\n",
    "            return extra in ('s', 'e', 'n', 'en')\n",
    "        return len(extra) <= max_extra_len(base_len)\n",
    "\n",
    "    def is_blocked_pair(base, word):\n",
    "        # Prevent 'wand'/'wande' from pulling in 'wandel*' variants.\n",
    "        if base in ('wand', 'wande') and word.startswith('wandel'):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    for base in unique_words:\n",
    "        if base in assigned:\n",
    "            continue\n",
    "        base_len = len(base)\n",
    "        group = [base]\n",
    "\n",
    "        for word in unique_words:\n",
    "            if word == base:\n",
    "                continue\n",
    "            if is_blocked_pair(base, word) or is_blocked_pair(word, base):\n",
    "                continue\n",
    "            if word.startswith(base):\n",
    "                extra = word[base_len:]\n",
    "                if allowed_small_suffix(extra, base_len):\n",
    "                    group.append(word)\n",
    "            elif base.startswith(word):\n",
    "                extra = base[len(word):]\n",
    "                if allowed_small_suffix(extra, len(word)):\n",
    "                    group.append(word)\n",
    "\n",
    "        if len(group) < 2:\n",
    "            assigned.add(base)\n",
    "            continue\n",
    "        group = sorted(set(group), key=lambda w: (len(w), w))\n",
    "        key = group[0]\n",
    "        candidates[key] = group\n",
    "        assigned.update(group)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def build_lemma_candidates(prefixes, suffixes, min_len=3):\n",
    "    words = pd.concat([\n",
    "        pd.Series(prefixes, dtype='string'),\n",
    "        pd.Series(suffixes, dtype='string'),\n",
    "    ])\n",
    "    return build_lemma_candidates_from_words(words, min_len=min_len)\n",
    "\n",
    "def merge_overlapping_candidates(candidates):\n",
    "    def choose_key(words):\n",
    "        return sorted(words, key=lambda w: (len(w), w))[0]\n",
    "\n",
    "    unvisited = set(candidates.keys())\n",
    "    merged = {}\n",
    "\n",
    "    while unvisited:\n",
    "        start = unvisited.pop()\n",
    "        stack = [start]\n",
    "        component = set()\n",
    "\n",
    "        while stack:\n",
    "            key = stack.pop()\n",
    "            if key in component:\n",
    "                continue\n",
    "            component.add(key)\n",
    "            for word in candidates.get(key, []):\n",
    "                if word in candidates and word not in component:\n",
    "                    stack.append(word)\n",
    "                    if word in unvisited:\n",
    "                        unvisited.remove(word)\n",
    "\n",
    "        all_words = set()\n",
    "        for key in component:\n",
    "            all_words.update(candidates.get(key, []))\n",
    "            all_words.add(key)\n",
    "        all_words = sorted(all_words)\n",
    "        merged_key = choose_key(all_words)\n",
    "        merged[merged_key] = all_words\n",
    "\n",
    "    return merged\n",
    "\n",
    "lemma_candidates = build_lemma_candidates(context['prefix'], context['suffix'])\n",
    "lemma_candidates = merge_overlapping_candidates(lemma_candidates)\n",
    "len(lemma_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few candidate groups for manual cleanup\n",
    "dict(list(lemma_candidates.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Lemmatization\n",
    "\n",
    "The dictionary replaces a variant suffix to a canonical lemma and writes it into suffix_lemma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reverse lookup so every variant points to its shortest lemma key\n",
    "suffix_lemma_map = {\n",
    "    variant: lemma\n",
    "    for lemma, variants in lemma_candidates.items()\n",
    "    for variant in variants\n",
    "}\n",
    "\n",
    "context['suffix_lemma'] = (\n",
    "    context['suffix'].map(suffix_lemma_map)\n",
    "    .fillna(context['suffix'])\n",
    ")\n",
    "\n",
    "context[['suffix', 'suffix_lemma']].head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness and Class Shares (After Lemmatization)\n",
    "\n",
    "We recompute the same tables after lemmatization for easy charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_stats_after = (\n",
    "    context['suffix_lemma']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('suffix_lemma')\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "suffix_stats_after['share'] = suffix_stats_after['count'] / suffix_stats_after['count'].sum()\n",
    "\n",
    "suffix_stats_after.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_uniqueness_after = pd.DataFrame({\n",
    "    'total_rows': [len(context)],\n",
    "    'unique_suffix_lemmas': [context['suffix_lemma'].nunique(dropna=False)],\n",
    "})\n",
    "\n",
    "suffix_uniqueness_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reuse the normalized data later, you can save it as a CSV or store it in a new table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed context to a new table in the same DB using pylib utils\n",
    "from handle_sqlite import save_dataframe_to_db\n",
    "\n",
    "# Choose a new table name to avoid overwriting the original 'context'\n",
    "output_table = 'context_processed'\n",
    "\n",
    "# Use 'replace' so re-running this cell updates the processed table cleanly\n",
    "save_dataframe_to_db(context, output_table, db_path, if_exists='replace')\n",
    "print(f\"Wrote DataFrame to table '{output_table}' in {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
