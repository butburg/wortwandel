{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing: Cutting from news Crawler Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if \"__file__\" in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "\n",
    "\n",
    "# Keep only data on/after the crawler change (use string comparison)\n",
    "CUTOFF = \"2022-04-21\"\n",
    "\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")\n",
    "context = read_table_as_dataframe(\"context\", db_path)\n",
    "\n",
    "# Load metadata to get publish dates (each newspaper_id is per newspaper+day)\n",
    "metadata = read_table_as_dataframe(\"newspapers\", db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata))\n",
    "print(len(context))\n",
    "metadata['data_published'].min(), metadata['data_published'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter metadata to to data\n",
    "metadata = metadata[metadata[\"data_published\"] >= CUTOFF].copy()\n",
    "# Filter context to the valid newspaper_ids\n",
    "valid_ids = set(metadata[\"newspaper_id\"])\n",
    "context = context[context[\"newspaper_id\"].isin(valid_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata))\n",
    "print(len(context))\n",
    "metadata['data_published'].min(), metadata['data_published'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write both filtered tables to new tables in the same DB\n",
    "from handle_sqlite import save_dataframe_to_db\n",
    "\n",
    "# Save filtered context\n",
    "save_dataframe_to_db(context, 'context_processed', db_path, if_exists='replace')\n",
    "print(f\"Wrote {len(context)} rows to 'context_processed'\")\n",
    "\n",
    "# Save filtered metadata\n",
    "save_dataframe_to_db(metadata, 'newspapers_processed', db_path, if_exists='replace')\n",
    "print(f\"Wrote {len(metadata)} rows to 'newspapers_processed'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing: Lemmatization and Lowercasing\n",
    "\n",
    "### What the Notebook Does\n",
    "\n",
    "- **Load Context Data:**  \n",
    "  Read the `context` table from the DWH database and focus on the text columns we want to normalize.\n",
    "\n",
    "- **Normalize Text:**  \n",
    "  Convert `pre_context`, `post_context`, `prefix`, and `suffix` to lowercase for consistent analysis.\n",
    "\n",
    "- **Manual Suffix Lemmatization:**  \n",
    "  Apply a curated mapping (created from a manual check) to normalize suffix variants into a common lemma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zweck dieses Notebooks und Verbindung zur Studienarbeit\n",
    "\n",
    "Dieses Notebook führt eine gezielte Textnormalisierung durch: Einfache Kleinschreibung und eine konservative, regelbasierte Zusammenführung morphologisch verwandter Präfix-/Suffix-Varianten (Lemmatisierung).\n",
    "\n",
    "Für die Studienarbeit leitet sich daraus die folgende Struktur ab:\n",
    "- **Daten & Methoden:** Dokumentation der EDA-Funde (z. B. orthografische Varianten von Klima-Komposita).\n",
    "- **Datenaufbereitung / Textnormalisierung:** Begründung der Lemmatisierung als methodische Entscheidung (nicht als Korrektur).\n",
    "- **Ergebnisse:** Vorher/Nachher-Vergleiche (Anzahl unique Begriffe, Klassenverteilungen).\n",
    "\n",
    "Die nachfolgenden Codezellen sind so kommentiert, dass sie sowohl den Algorithmus als auch die Annahmen (konservative Suffix-Regeln, Merge-Logik) für die schriftliche Beschreibung klar machen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if \"__file__\" in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "import pandas as pd\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Context Data\n",
    "\n",
    "We load the `context` table and focus on the text columns we want to normalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = read_table_as_dataframe(\"context_processed\", db_path)\n",
    "\n",
    "text_columns = [\"pre_context\", \"post_context\", \"prefix\", \"suffix\"]\n",
    "context[text_columns] = context[text_columns].astype(\"string\")\n",
    "context.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase All Text Columns\n",
    "\n",
    "This ensures consistent casing before any manual or automated normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in text_columns:\n",
    "    context[column] = context[column].str.lower()\n",
    "\n",
    "context[text_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness and Class Shares (Before Lemmatization)\n",
    "\n",
    "We summarize uniqueness and relative shares of suffix classes before lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_stats_before = (\n",
    "    context['suffix']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('suffix')\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "suffix_stats_before['share'] = suffix_stats_before['count'] / suffix_stats_before['count'].sum()\n",
    "\n",
    "suffix_stats_before.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare rank (x) and relative frequency (y) for Zipf plot (limit to top 70)\n",
    "s = suffix_stats_before.sort_values('count', ascending=False).reset_index(drop=True).head(70)\n",
    "ranks = s.index + 1\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "# Linear plot (no log scale)\n",
    "plt.plot(ranks, s['share'].values, marker='.')\n",
    "plt.xlim(1, 70)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "fmt = FuncFormatter(lambda v, pos: f\"{v:0.2f}\".replace('.',','))\n",
    "plt.gca().yaxis.set_major_formatter(fmt)\n",
    "plt.xlabel('Rang (1–70)')\n",
    "plt.ylabel('Relative Worthäufigkeit')\n",
    "plt.title('Zipf: Suffixverteilung (vor der Lemmatisierung)')\n",
    "plt.grid(True, which='both', ls='--', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_uniqueness_before = pd.DataFrame({\n",
    "    'total_rows': [len(context)],\n",
    "    'unique_suffixes': [context['suffix'].nunique(dropna=False)],\n",
    "})\n",
    "\n",
    "suffix_uniqueness_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lemma Candidates from Prefixes and Suffixes\n",
    "\n",
    "We generate candidate lemma groups using conservative German suffix rules.\n",
    "Short bases only allow small plural-style endings (e.g., +s, +n, +en),\n",
    "while longer bases allow limited extra length (about 25%).\n",
    "Overlapping groups are merged so a key appearing in another group pulls the sets together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Algorithm overview (expanded):\n",
    "# - Goal: Group orthographic/morphological variants of short lexical items so we can pick a\n",
    "# - Constraints: Use conservative rules to avoid over-merging unrelated stems (important in German).\n",
    "# - Steps: 1) generate candidate groups by length/suffix heuristics; 2) choose shortest as key;\n",
    "#          3) merge groups that overlap because a variant can itself be a key in another group.\n",
    "\n",
    "def build_lemma_candidates_from_words(words, min_len=3):\n",
    "    \"\"\"Build conservative lemma candidate groups from an iterable of words.\n",
    "\n",
    "    Parameters:\n",
    "    - words: iterable of strings (prefixes and/or suffixes).\n",
    "    - min_len: minimum base length to consider (filters out very short tokens).\n",
    "\n",
    "    Returns:\n",
    "    - dict: {lemma_candidate: [variants,...]} where lemma_candidate is the shortest form in the group.\n",
    "\"\"\"\n",
    "    words = pd.Series(words, dtype='string')\n",
    "    # normalize input: drop missing, trim whitespace, lowercase (we ran lower earlier but be safe)\n",
    "    words = words.dropna().str.strip().str.lower()\n",
    "    words = words[words.str.len() >= min_len]\n",
    "\n",
    "    # sort by length then alphabetically so shorter candidates are considered first\n",
    "    unique_words = sorted(set(words.tolist()), key=lambda w: (len(w), w))\n",
    "    assigned = set()  # words already assigned to a candidate group\n",
    "    candidates = {}\n",
    "\n",
    "    def max_extra_len(base_len):\n",
    "        # Allowed extra length for variants relative to the base length.\n",
    "        if base_len <= 3:\n",
    "            return 1\n",
    "        if base_len == 4:\n",
    "            return 2\n",
    "        return max(1, math.ceil(base_len * 0.25))\n",
    "\n",
    "    def allowed_small_suffix(extra, base_len):\n",
    "        # Heuristic rules for allowed endings to avoid merging unrelated tokens.\n",
    "        if base_len <= 3:\n",
    "            # very short bases: only allow simple +s plural-like forms\n",
    "            return extra == 's'\n",
    "        if base_len == 4:\n",
    "            # 4-letter bases: small set of short flexions are permitted\n",
    "            return extra in ('s', 'e', 'n', 'en')\n",
    "        # longer bases: allow up to ~25% extra length (rounded up)\n",
    "        return len(extra) <= max_extra_len(base_len)\n",
    "\n",
    "    def is_blocked_pair(base, word):\n",
    "        # Specific blocklist entries to prevent known bad merges. Keep small and explicit.\n",
    "        if base in ('wand', 'wande') and word.startswith('wandel'):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    for base in unique_words:\n",
    "        if base in assigned:\n",
    "            continue\n",
    "        base_len = len(base)\n",
    "        group = [base]\n",
    "\n",
    "        for word in unique_words:\n",
    "            if word == base:\n",
    "                continue\n",
    "            # avoid explicit blocked merges in either direction\n",
    "            if is_blocked_pair(base, word) or is_blocked_pair(word, base):\n",
    "                continue\n",
    "            # check if 'word' extends 'base' by a short allowed suffix\n",
    "            if word.startswith(base):\n",
    "                extra = word[base_len:]\n",
    "                if allowed_small_suffix(extra, base_len):\n",
    "                    group.append(word)\n",
    "            # symmetric: base might extend a shorter candidate 'word'\n",
    "            elif base.startswith(word):\n",
    "                extra = base[len(word):]\n",
    "                if allowed_small_suffix(extra, len(word)):\n",
    "                    group.append(word)\n",
    "\n",
    "        # only accept groups with at least two members (otherwise treat base as standalone)\n",
    "        if len(group) < 2:\n",
    "            assigned.add(base)\n",
    "            continue\n",
    "        group = sorted(set(group), key=lambda w: (len(w), w))\n",
    "        key = group[0]  # shortest form chosen as lemma key\n",
    "        candidates[key] = group\n",
    "        assigned.update(group)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def build_lemma_candidates(prefixes, suffixes, min_len=3):\n",
    "    \"\"\"Convenience wrapper: build candidates from two series (prefixes and suffixes).\"\"\"\n",
    "    words = pd.concat([\n",
    "        pd.Series(prefixes, dtype='string'),\n",
    "        pd.Series(suffixes, dtype='string'),\n",
    "    ])\n",
    "    return build_lemma_candidates_from_words(words, min_len=min_len)\n",
    "\n",
    "def merge_overlapping_candidates(candidates):\n",
    "    \"\"\"Merge candidate groups that overlap into connected components.\n",
    "\n",
    "    Rationale: A variant in one group may itself be the key of another group; those need\n",
    "\n",
    "    Returns a new dict mapping the chosen merged key (shortest word) to the full set.\n",
    "    \"\"\"\n",
    "    def choose_key(words):\n",
    "        # pick the shortest alphabetical minimal key for deterministic results\n",
    "        return sorted(words, key=lambda w: (len(w), w))[0]\n",
    "\n",
    "    unvisited = set(candidates.keys())\n",
    "    merged = {}\n",
    "\n",
    "    while unvisited:\n",
    "        start = unvisited.pop()\n",
    "        stack = [start]\n",
    "        component = set()\n",
    "\n",
    "        while stack:\n",
    "            key = stack.pop()\n",
    "            if key in component:\n",
    "                continue\n",
    "            component.add(key)\n",
    "            # traverse edges: for each word in the group's variants, if that word is also a key,\n",
    "            # follow it to include its variants as part of the same connected component\n",
    "            for word in candidates.get(key, []):\n",
    "                if word in candidates and word not in component:\n",
    "                    stack.append(word)\n",
    "                    if word in unvisited:\n",
    "                        unvisited.remove(word)\n",
    "\n",
    "        # collect all words from the component's groups\n",
    "        all_words = set()\n",
    "        for key in component:\n",
    "            all_words.update(candidates.get(key, []))\n",
    "            all_words.add(key)\n",
    "        all_words = sorted(all_words)\n",
    "        merged_key = choose_key(all_words)\n",
    "        merged[merged_key] = all_words\n",
    "\n",
    "    return merged\n",
    "\n",
    "# Build and merge candidates from the dataframe columns\n",
    "lemma_candidates = build_lemma_candidates(context['prefix'], context['suffix'])\n",
    "lemma_candidates = merge_overlapping_candidates(lemma_candidates)\n",
    "len(lemma_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few candidate groups for manual cleanup\n",
    "dict(list(lemma_candidates.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Lemmatization\n",
    "\n",
    "The dictionary replaces a variant suffix to a canonical lemma and writes it into suffix_lemma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reverse lookup so every variant points to its shortest lemma key\n",
    "suffix_lemma_map = {\n",
    "    variant: lemma\n",
    "    for lemma, variants in lemma_candidates.items()\n",
    "    for variant in variants\n",
    "}\n",
    "\n",
    "context['suffix_lemma'] = (\n",
    "    context['suffix'].map(suffix_lemma_map)\n",
    "    .fillna(context['suffix'])\n",
    ")\n",
    "\n",
    "context[['suffix', 'suffix_lemma']].head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness and Class Shares (After Lemmatization)\n",
    "\n",
    "We recompute the same tables after lemmatization for easy charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_stats_after = (\n",
    "    context['suffix_lemma']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('suffix_lemma')\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "suffix_stats_after['share'] = suffix_stats_after['count'] / suffix_stats_after['count'].sum()\n",
    "\n",
    "suffix_stats_after.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare rank (x) and relative frequency (y) for Zipf plot after lemmatization (limit to top 70)\n",
    "s = suffix_stats_after.sort_values('count', ascending=False).reset_index(drop=True).head(70)\n",
    "ranks = s.index + 1\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "# Linear plot (no log scale)\n",
    "plt.plot(ranks, s['share'].values, marker='.')\n",
    "plt.xlim(1, 70)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "fmt = FuncFormatter(lambda v, pos: f\"{v:0.2f}\".replace('.',','))\n",
    "plt.gca().yaxis.set_major_formatter(fmt)\n",
    "plt.xlabel('Rang (1–70)')\n",
    "plt.ylabel('Relative Worthäufigkeit')\n",
    "plt.title('Zipf: Suffixverteilung (nach der Lemmatisierung)')\n",
    "plt.grid(True, which='both', ls='--', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_uniqueness_after = pd.DataFrame({\n",
    "    'total_rows': [len(context)],\n",
    "    'unique_suffix_lemmas': [context['suffix_lemma'].nunique(dropna=False)],\n",
    "})\n",
    "\n",
    "suffix_uniqueness_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reuse the normalized data later, you can save it as a CSV or store it in a new table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed context to a new table in the same DB using pylib utils\n",
    "from handle_sqlite import save_dataframe_to_db\n",
    "\n",
    "# Choose a new table name to avoid overwriting the original 'context'\n",
    "output_table = 'context_processed'\n",
    "\n",
    "# Use 'replace' so re-running this cell updates the processed table cleanly\n",
    "save_dataframe_to_db(context, output_table, db_path, if_exists='replace')\n",
    "print(f\"Wrote DataFrame to table '{output_table}' in {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the processed data as CSV files with today's date in filenames\n",
    "output_dir = os.path.join(notebook_dir, \"..\", \"data_output\")\n",
    "metadata.to_csv(os.path.join(output_dir, f\"dwh_meta_processed_{today}.csv\"), index=False)\n",
    "context.to_csv(os.path.join(output_dir, f\"dwh_context_processed_{today}.csv\"), index=False)\n",
    "\n",
    "print(f\"Exported CSV files with date {today}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
