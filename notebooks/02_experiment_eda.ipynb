{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for path mainly\n",
    "import sys # for path mainly\n",
    "import logging # for status and log output\n",
    "import matplotlib.pyplot as plt # for plotting charts\n",
    "import seaborn as sns # for plotting default nice charts\n",
    "\n",
    "# Ensure path resolves relative to notebook file (robust in different CWDs)\n",
    "notebook_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    ")\n",
    "sys.path.append(os.path.normpath(os.path.join(notebook_dir, \"..\", \"pylib\")))\n",
    "\n",
    "\n",
    "from handle_sqlite import save_dataframe_to_db, read_table_as_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic + fix (run in a notebook cell)\n",
    "import os, sys\n",
    "notebook_dir = (os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd())\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"notebook_dir:\", notebook_dir)\n",
    "\n",
    "pylib_path = os.path.normpath(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "db_path = os.path.normpath(os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\"))\n",
    "print(\"pylib exists?:\", os.path.exists(pylib_path), pylib_path)\n",
    "print(\"db exists?:\", os.path.exists(db_path), db_path)\n",
    "\n",
    "# Option A: set CWD to project root so relative paths work\n",
    "project_root = os.path.normpath(os.path.join(notebook_dir, \"..\"))\n",
    "os.chdir(project_root)\n",
    "print(\"new CWD:\", os.getcwd())\n",
    "\n",
    "# Option B (recommended): use absolute paths when calling helpers:\n",
    "# metadata = read_table_as_dataframe(\"newspapers\", db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "load data from dhw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from dwh the newspaper informations per date and their usage of klima\n",
    "metadata = read_table_as_dataframe(\"newspapers\", \"data_output/dwh_data.db\")\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from dwh the found klima words\n",
    "context = read_table_as_dataframe(\"context\", \"data_output/dwh_data.db\")\n",
    "context.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "cast types can help analysis and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = context.astype({'pre_context': 'string',\n",
    "                         'post_context': 'string',\n",
    "                         'prefix': 'string',\n",
    "                         'suffix': 'string',})\n",
    "context.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata.newspaper_name == 'heise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dublicated newspapers published\n",
    "metadata.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dublicates of context, including newspaper id. indicates static use\n",
    "# of klima, like navigation term of a paper.\n",
    "context.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of news pages (not newspaper-companys) with at least once\n",
    "# klima in actual data\n",
    "context.newspaper_id.nunique() / len(context) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% of newspaper-companys, that never mentioned klima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.groupby(['newspaper_name'])['klima_mentions_count'].sum()\n",
    "klima_counts_per_company = metadata.pivot_table(values='klima_mentions_count', index='newspaper_name', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_companys = len(klima_counts_per_company)\n",
    "total_companys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_klima_companies = (klima_counts_per_company['klima_mentions_count'] == 0).sum()\n",
    "no_klima_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_klima_companies / total_companys * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(klima_counts_per_company_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the times, klima is used in one single paper publish\n",
    "ax = sns.countplot(data=metadata, x=\"klima_mentions_count\", palette=\"flare\")\n",
    "#ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the lower freq edges\n",
    "\n",
    "top_three = sorted(metadata['klima_mentions_count'].unique())[:5]\n",
    "counts = metadata['klima_mentions_count'].value_counts().loc[top_three]\n",
    "\n",
    "ax = sns.barplot(y=counts.index, x=counts.values, orient='h', palette=\"flare\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the higher freq edges and its newspapers\n",
    "\n",
    "top_three = sorted(metadata['klima_mentions_count'].unique())[-5:]\n",
    "counts = metadata['klima_mentions_count'].value_counts().loc[top_three]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=context, x=\"newspaper_id\", order=context.newspaper_id.value_counts().iloc[:5].index, palette=\"flare\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=context, x=\"newspaper_id\", order=context.newspaper_id.value_counts().iloc[:15].index, palette=\"flare\")\n",
    "#ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging for deeper eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the one to many relationship meets expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows for each newspaper_id in the metadata and context tables\n",
    "metadata_counts = metadata['newspaper_id'].value_counts()\n",
    "\n",
    "# Check if there are any newspaper_ids that appear more than once in the metadata table\n",
    "problematic_metadata = metadata_counts[metadata_counts > 1]\n",
    "\n",
    "print(f\"Problematic newspaper_id in metadata (appears more than once):\\n{problematic_metadata}\")\n",
    "\n",
    "# Check for any cases where there are no rows in metadata for context's newspaper_id\n",
    "missing_metadata = context[~context['newspaper_id'].isin(metadata['newspaper_id'])]\n",
    "print(f\"Context rows with missing metadata:\\n{missing_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[~context['newspaper_id'].isin(metadata['newspaper_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first few rows of both dataframes\n",
    "print(metadata.head(2))\n",
    "print(context.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for one-to-many relationship based on 'newspaper_id'\n",
    "merged = pd.merge(context, metadata, on=\"newspaper_id\", how=\"inner\")\n",
    "\n",
    "# Checking the number of unique newspaper_id in both tables\n",
    "print(f\"Unique newspaper_id in metadata: {metadata['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in context: {context['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in merged: {merged['newspaper_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(merged['data_published'], errors='coerce')\n",
    "dates.min(), dates.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert data_published to datetime if not already done\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'], errors='coerce')\n",
    "\n",
    "# Get min/max dates per newspaper to understand coverage\n",
    "coverage = merged.groupby('newspaper_name')['data_published'].agg(['min', 'max', 'count']).reset_index()\n",
    "coverage.columns = ['newspaper_name', 'first_date', 'last_date', 'total_records']\n",
    "coverage = coverage.sort_values('first_date')\n",
    "\n",
    "print(coverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Ensure datetime\n",
    "metadata['data_published'] = pd.to_datetime(metadata['data_published'], errors='coerce')\n",
    "metadata['has_klima'] = metadata['klima_mentions_count'] > 0\n",
    "\n",
    "# Sort newspapers by first appearance\n",
    "newspapers = (\n",
    "    metadata.groupby('newspaper_name')['data_published'].min()\n",
    "    .sort_values()\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "date_min = metadata['data_published'].min()\n",
    "date_max = metadata['data_published'].max()\n",
    "all_dates = pd.date_range(start=date_min, end=date_max, freq='D')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "y_pos = 0\n",
    "for np_name in newspapers:\n",
    "    np_meta = metadata[metadata['newspaper_name'] == np_name]\n",
    "    crawled_dates = set(np_meta['data_published'].dt.date)\n",
    "    klima_dates = set(np_meta[np_meta['has_klima']]['data_published'].dt.date)\n",
    "\n",
    "    for d in all_dates:\n",
    "        if d.date() in klima_dates:\n",
    "            ax.barh(y_pos, 1, left=d, height=0.8, color='steelblue', edgecolor='none')\n",
    "        elif d.date() in crawled_dates:\n",
    "            ax.barh(y_pos, 1, left=d, height=0.8, color='lightgray', edgecolor='none')\n",
    "    y_pos += 1\n",
    "\n",
    "ax.set_yticks(range(len(newspapers)))\n",
    "ax.set_yticklabels(newspapers, fontsize=9)\n",
    "ax.set_xlabel('Datum Veröffentlicht', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Newspaper Name', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Gecrawlte Daten: Blau=Klima, Grau=Ohne Klima', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%d. %b. %y'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.set_xlim(date_min, date_max)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "\n",
    "# Marker for crawler change on 01. April 2022\n",
    "crawler_change = pd.Timestamp('2022-04-21')\n",
    "ax.axvline(crawler_change, color='crimson', linestyle='--', linewidth=1.2, alpha=0.9, label='Crawler-Änderung 01.04.2022')\n",
    "\n",
    "# Optional legend\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
