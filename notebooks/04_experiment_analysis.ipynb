{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse klima word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# make helper methods available\n",
    "# Add custom library path relative to notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"pylib\"))\n",
    "\n",
    "\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "\n",
    "db_path = os.path.join(notebook_dir, \"..\", \"data_output\", \"dwh_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Seaborn settings\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from dhw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from dwh the newspaper informations per date and their usage of klima\n",
    "metadata = read_table_as_dataframe(\"newspapers\", db_path)\n",
    "display(metadata.head(3))\n",
    "\n",
    "# load from dwh the found klima words\n",
    "context = read_table_as_dataframe(\"context\", db_path)\n",
    "display(context.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cast types can help analysis and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = context.astype({'pre_context': 'string',\n",
    "                         'post_context': 'string',\n",
    "                         'prefix': 'string',\n",
    "                         'suffix': 'string',})\n",
    "context.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging for deeper eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again data sanity\n",
    "context[~context['newspaper_id'].isin(metadata['newspaper_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes on 'newspaper_id'\n",
    "merged = pd.merge(context, metadata, on=\"newspaper_id\", how=\"inner\")\n",
    "\n",
    "# Checking the number of unique newspaper_id in both tables\n",
    "print(f\"Unique newspaper_id in metadata: {metadata['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in context: {context['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in merged: {merged['newspaper_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime for proper time-based aggregation\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed context table (with 'suffix_lemma') if available, else fall back to 'context'\n",
    "try:\n",
    "    context_proc = read_table_as_dataframe(\"context_processed\", db_path)\n",
    "    print(\"Loaded table 'context_processed'\")\n",
    "except Exception as e:\n",
    "    print('context_processed not found, falling back to context table:', e)\n",
    "    context_proc = read_table_as_dataframe(\"context\", db_path)\n",
    "\n",
    "# Ensure string dtypes for relevant columns\n",
    "for c in ['pre_context','post_context','prefix','suffix','suffix_lemma']:\n",
    "    if c in context_proc.columns:\n",
    "        context_proc[c] = context_proc[c].astype('string')\n",
    "\n",
    "# Merge with metadata to get publication dates (if not already present)\n",
    "if 'newspaper_id' in context_proc.columns and 'newspaper_id' in metadata.columns:\n",
    "    merged_proc = pd.merge(context_proc, metadata, on='newspaper_id', how='inner')\n",
    "else:\n",
    "    merged_proc = context_proc.copy()\n",
    "\n",
    "# Ensure we have a datetime column named 'data_published'\n",
    "if 'data_published' in merged_proc.columns:\n",
    "    merged_proc['data_published'] = pd.to_datetime(merged_proc['data_published'])\n",
    "else:\n",
    "    raise ValueError('No data_published column available after merging; cannot plot time series')\n",
    "\n",
    "# Target lemmas to plot (lowercase)\n",
    "lemmas = ['schutz', 'wandel', 'krise']\n",
    "\n",
    "# Build a daily count series for each lemma (then aggregate weekly)\n",
    "counts = {}\n",
    "for lemma in lemmas:\n",
    "    mask = pd.Series(False, index=merged_proc.index)\n",
    "    # exact matches in lemma columns\n",
    "    for col in ['suffix_lemma','prefix','suffix']:\n",
    "        if col in merged_proc.columns:\n",
    "            mask = mask | (merged_proc[col].fillna('').str.lower() == lemma)\n",
    "    # word-boundary search in context fields\n",
    "    for col in ['pre_context','post_context']:\n",
    "        if col in merged_proc.columns:\n",
    "            mask = mask | merged_proc[col].fillna('').str.contains(fr'\\b{lemma}\\b', case=False, regex=True)\n",
    "    # daily counts (index by datetime)\n",
    "    s = merged_proc.loc[mask].set_index('data_published').resample('D').size()\n",
    "    counts[lemma] = s\n",
    "\n",
    "# Combine into a DataFrame (daily) and then aggregate to monthly (Monatsbeginn)\n",
    "counts_df = pd.concat(counts, axis=1).sort_index()\n",
    "monthly = counts_df.resample('MS').sum()\n",
    "\n",
    "# Build full monthly index (Monatsbeginn) but keep NaN for months with no data to show gaps\n",
    "full_months = pd.date_range(start=monthly.index.min(), end=monthly.index.max(), freq='MS')\n",
    "monthly = monthly.reindex(full_months)\n",
    "monthly.index = pd.to_datetime(monthly.index)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plot monthly series without smoothing; missing months remain as gaps (NaN)\n",
    "plt.figure(figsize=(12,6))\n",
    "for lemma in lemmas:\n",
    "    if lemma in monthly.columns:\n",
    "        sns.lineplot(x=monthly.index, y=monthly[lemma], label=lemma)\n",
    "\n",
    "# Lineare Trendlinien pro Lemma (auf Monatsdaten)\n",
    "num_points = len(monthly.index)\n",
    "x_full = np.arange(num_points)\n",
    "for lemma in lemmas:\n",
    "    series = monthly[lemma]\n",
    "    valid = series.dropna()\n",
    "    if len(valid) >= 2:\n",
    "        pos = valid.index.map(lambda idx: monthly.index.get_loc(idx)).to_numpy()\n",
    "        coeffs = np.polyfit(pos, valid.to_numpy(), 1)\n",
    "        trend = np.polyval(coeffs, x_full)\n",
    "        plt.plot(monthly.index, trend, linestyle='--', linewidth=1.5, label=f\"{lemma} Trend\")\n",
    "\n",
    "# Mark key dates: Feb 2022 (Krisenpeak) and Hitzewelle Mitte Juli 2022\n",
    "for date, label in [('2022-04-21', 'Feb 2022'), ('2022-07-15', 'Hitzewelle Juli 2022')]:\n",
    "    plt.axvline(pd.to_datetime(date), color='red', linestyle='--', alpha=0.6)\n",
    "    plt.text(pd.to_datetime(date), plt.ylim()[1]*0.9, label, rotation=90, color='red', va='top', ha='right', fontsize=9)\n",
    "\n",
    "plt.xlabel('Datum (Monatsbeginn)')\n",
    "plt.ylabel('Anzahl Nennungen pro Monat')\n",
    "plt.title('Monatliche Nennungen: schutz, wandel, krise (Monatsaggregation, Lücken bleiben)')\n",
    "plt.legend(title='Lemma')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestapeltes 100%-Balkendiagramm der quartalsweisen Nennungen\n",
    "quarterly_bar = monthly.fillna(0).resample('QS').sum()\n",
    "full_quarters = pd.date_range(start=quarterly_bar.index.min(), end=quarterly_bar.index.max(), freq='QS')\n",
    "quarterly_bar = quarterly_bar.reindex(full_quarters)\n",
    "quarterly_bar.index = pd.to_datetime(quarterly_bar.index)\n",
    "\n",
    "totals = quarterly_bar.sum(axis=1)\n",
    "quarterly_pct = quarterly_bar.div(totals.replace(0, pd.NA), axis=0) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "quarterly_pct.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_xlabel('Datum (Quartalsbeginn)')\n",
    "ax.set_ylabel('Anteil pro Quartal (%)')\n",
    "ax.set_title('Quartalsweise Nennungen: schutz, wandel, krise (gestapeltes 100%-Balkendiagramm)')\n",
    "ax.legend(title='Lemma')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestapeltes 100%-Balkendiagramm der monatlichen Nennungen\n",
    "monthly_bar = monthly.fillna(0)\n",
    "totals = monthly_bar.sum(axis=1)\n",
    "monthly_pct = monthly_bar.div(totals.replace(0, pd.NA), axis=0) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "monthly_pct.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_xlabel('Datum (Monatsbeginn)')\n",
    "ax.set_ylabel('Anteil pro Monat (%)')\n",
    "ax.set_title('Monatliche Nennungen: schutz, wandel, krise (gestapeltes 100%-Balkendiagramm)')\n",
    "ax.legend(title='Lemma')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add maybe klima relevant dates and other big main events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined global and German-specific climate events (post 2021-04-01)\n",
    "events = {\n",
    "    'Urteil des Bundesverfassungsgerichts zum Klimaschutz': '2021-04-29',     #+ Federal Court ruling mandating stronger climate measures\n",
    "    'Bundestag beschließt neues Bundes-Klimaschutzgesetz (KSG)':'2021-06-24',\n",
    "    'IPCC AR6 Bericht (WG1)': '2021-08-06',                          #+ Release of the first working group report of AR6\n",
    "    'Deutsche Bundestagswahl': '2021-09-26',                        #+ Election impacting national climate policy direction\n",
    "    'COP26 Glasgow & Global Methane Pledge': '2021-11-01',             #+ COP26 summit held in Glasgow\n",
    "    'Beginn Ukraine-Russland-Krieg': '2022-02-24',            #+ Major geopolitical event affecting global energy and climate debate\n",
    "    'G7-Gipfel in Schloss Elmau': '2022-06-26',             #+ Key domestic conference on climate strategy and policy\n",
    "    'COP27 (UN-Klimagipfel)': '2022-11-06',             #+ COP27 summit held in Sharm El-Sheikh\n",
    "    'IPCC AR6 Synthesebericht': '2023-03-20',             #+ Comprehensive synthesis of the AR6 findings\n",
    "    'Änderung Bundes-Klimaschutzgesetz': '2023-10-04',          # Update to national climate legislation following legal mandates\n",
    "    'COP28 (UN-Klimagipfel)': '2023-11-30',             #+ UN Climate Change Conference - United Arab Emirates\n",
    "    'COP29 (UN-Klimagipfel)': '2024-11-11',             #+ UN Climate Change Conference Baku - November 2024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-step for Sentiment Analysis: Define a helper function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to the 'pre_context' and 'post_context' columns\n",
    "merged['sentiment_pre'] = merged['pre_context'].apply(get_sentiment)\n",
    "merged['sentiment_post'] = merged['post_context'].apply(get_sentiment)\n",
    "# For simplicity, combine the two sentiments (you may choose to refine this later)\n",
    "merged['sentiment'] = merged['sentiment_pre'] + ' ' + merged['sentiment_post']\n",
    "\n",
    "# Create a month-year period column for monthly aggregations\n",
    "merged['month_year'] = merged['data_published'].dt.to_period('M').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['sentiment_pre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['sentiment_post'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Trends in Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'data_published' is a datetime object\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Aggregate by month, summing the 'klima_mentions_count'\n",
    "monthly_trends = (\n",
    "    merged.groupby(merged['data_published'].dt.to_period('M'))['klima_mentions_count']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert period to string for plotting\n",
    "monthly_trends['data_published'] = monthly_trends['data_published'].astype(str)\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=monthly_trends, color='steelblue')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time (Monthly Aggregation)')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_trends = (\n",
    "    merged.groupby(merged['data_published'].dt.to_period('M'))['klima_mentions_count']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "# Convert 'data_published' from period[M] to string for plotting\n",
    "monthly_trends['month'] = monthly_trends['data_published'].dt.to_timestamp(how='end')\n",
    "\n",
    "\n",
    "\n",
    "# Plot monthly aggregation with automatic x-axis ticks\n",
    "ax = sns.lineplot(x='month', y='klima_mentions_count', data=monthly_trends, color='steelblue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Monthly Aggregation of Klima Mentions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Sort by 'data_published' to ensure dates are ordered correctly\n",
    "trends_in_frequency_sorted = trends_in_frequency.sort_values('data_published')\n",
    "\n",
    "# Plot the data with dates on the x-axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_sorted, color='steelblue')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=merged['data_published'].min(), end=merged['data_published'].max(), freq='D')\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Merge the trends with the complete date range, filling missing dates with 0\n",
    "trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "# Plot the data with dates on the x-axis, treating them as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, color='steelblue')\n",
    "\n",
    "# Format the x-axis to show dates correctly, adjusting tick spacing\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=merged['data_published'].min(), end=merged['data_published'].max(), freq='D')\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Merge the trends with the complete date range, filling missing dates with 0\n",
    "trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "# Extract month and year from 'data_published'\n",
    "trends_in_frequency_full['month_year'] = trends_in_frequency_full['data_published'].dt.to_period('M')\n",
    "\n",
    "# Group by 'month_year' and sum 'klima_mentions_count'\n",
    "monthly_trends = trends_in_frequency_full.groupby('month_year')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Plot the data with months on the x-axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='month_year', y='klima_mentions_count', data=monthly_trends, color='steelblue')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" by Month')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_comparison_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'newspaper_name' and sum the 'klima_mentions_count'\n",
    "media_comparison = merged.groupby('newspaper_name')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Sort by 'klima_mentions_count' to see the newspapers with the most mentions first\n",
    "media_comparison_sorted = media_comparison.sort_values('klima_mentions_count', ascending=False)\n",
    "\n",
    "# Plot the data using a horizontal bar plot\n",
    "plt.figure(figsize=(12, 16))\n",
    "sns.barplot(x='klima_mentions_count', y='newspaper_name', data=media_comparison_sorted[-4:], color='steelblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Total Klima Mentions')\n",
    "plt.ylabel('Newspaper Name')\n",
    "plt.title('Comparison of Klima Mentions Across Different Newspapers')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key external events (dates)\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency, color='steelblue')\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions and Correlation with External Events')\n",
    "#plt.legend(title=\"Key Events\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Sum 'klima_mentions_count' by newspaper\n",
    "top_newspapers = merged.groupby('newspaper_name')['klima_mentions_count'].sum().nlargest(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 newspapers\n",
    "top_newspapers_data = merged[merged['newspaper_name'].isin(top_newspapers)]\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_newspapers_data['data_published'].min(), end=top_newspapers_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each top newspaper\n",
    "plt.figure(figsize=(14, 8))\n",
    "for newspaper in top_newspapers:\n",
    "    newspaper_data = top_newspapers_data[top_newspapers_data['newspaper_name'] == newspaper]\n",
    "\n",
    "    # Group by date and sum 'klima_mentions_count'\n",
    "    trends_in_frequency = newspaper_data.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "    trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "    # Plot the time series line for the newspaper\n",
    "    sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, label=newspaper)\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions by Newspaper and Correlation with External Events')\n",
    "plt.legend(title=\"Key Events and Newspapers\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Sum 'klima_mentions_count' by newspaper\n",
    "top_newspapers = merged.groupby('newspaper_name')['klima_mentions_count'].sum().nlargest(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 newspapers\n",
    "top_newspapers_data = merged[merged['newspaper_name'].isin(top_newspapers)]\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_newspapers_data['data_published'].min(), end=top_newspapers_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each top newspaper\n",
    "plt.figure(figsize=(14, 8))\n",
    "for newspaper in top_newspapers:\n",
    "    newspaper_data = top_newspapers_data[top_newspapers_data['newspaper_name'] == newspaper]\n",
    "\n",
    "    # Group by date and sum 'klima_mentions_count'\n",
    "    trends_in_frequency = newspaper_data.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "    trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "    # Plot the time series line for the newspaper\n",
    "    sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, label=newspaper, markers=True, marker='o')\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions by Newspaper and Correlation with External Events')\n",
    "plt.legend(title=\"Key Events and Newspapers\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment analyese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Perform sentiment analysis on the 'pre_context' and 'post_context' columns\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to both 'pre_context' and 'post_context'\n",
    "merged['sentiment_pre'] = merged['pre_context'].apply(get_sentiment)\n",
    "merged['sentiment_post'] = merged['post_context'].apply(get_sentiment)\n",
    "\n",
    "# Combine both sentiments into a single column\n",
    "merged['sentiment'] = merged['sentiment_pre'] + ' ' + merged['sentiment_post']\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a date range (monthly, for example)\n",
    "merged['month_year'] = merged['data_published'].dt.to_period('M')\n",
    "\n",
    "# Count the frequency of sentiments and sum 'klima_mentions_count' for each period\n",
    "sentiment_data = merged.groupby(['month_year', 'sentiment'])['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Pivot the data to get separate columns for each sentiment type (positive, negative, neutral)\n",
    "sentiment_pivot = sentiment_data.pivot_table(index='month_year', columns='sentiment', values='klima_mentions_count', aggfunc='sum').fillna(0)\n",
    "\n",
    "# Plot the sentiment distribution over time (stacked bar plot)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sentiment_pivot.plot(kind='bar', stacked=True, figsize=(14, 8), color=['lightgreen', 'lightcoral', 'lightgrey'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Sentiment Distribution of \"Klima\" Mentions Over Time')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'suffix' and count occurrences\n",
    "suffix_counts = merged['suffix'].value_counts().head(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 suffixes\n",
    "top_suffix_data = merged[merged['suffix'].isin(suffix_counts)]\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "top_suffix_data['data_published'] = pd.to_datetime(top_suffix_data['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_suffix_data['data_published'].min(), end=top_suffix_data['data_published'].max(), freq='M')\n",
    "\n",
    "# Plot for each of the top 7 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "for suffix in suffix_counts:\n",
    "    # Filter the data for the current suffix\n",
    "    suffix_data = top_suffix_data[top_suffix_data['suffix'] == suffix]\n",
    "\n",
    "    # Group by date and count the occurrences of the suffix\n",
    "    suffix_trends = suffix_data.groupby('data_published').size().reset_index(name='count')\n",
    "\n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    suffix_trends_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    suffix_trends_full = suffix_trends_full.merge(suffix_trends, on='data_published', how='left')\n",
    "    suffix_trends_full['count'] = suffix_trends_full['count'].fillna(0)\n",
    "\n",
    "    # Plot the time series line for the suffix\n",
    "    sns.lineplot(x='data_published', y='count', data=suffix_trends_full, label=suffix)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 7 Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Suffix Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'suffix' and count occurrences\n",
    "suffix_counts = merged['suffix'].value_counts().head(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 suffixes\n",
    "top_suffix_data = merged[merged['suffix'].isin(suffix_counts)]\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "top_suffix_data['data_published'] = pd.to_datetime(top_suffix_data['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_suffix_data['data_published'].min(), end=top_suffix_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each of the top 7 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "for suffix in suffix_counts:\n",
    "    # Filter the data for the current suffix\n",
    "    suffix_data = top_suffix_data[top_suffix_data['suffix'] == suffix]\n",
    "\n",
    "    # Group by date and count the occurrences of the suffix\n",
    "    suffix_trends = suffix_data.groupby('data_published').size().reset_index(name='count')\n",
    "\n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    suffix_trends_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    suffix_trends_full = suffix_trends_full.merge(suffix_trends, on='data_published', how='left')\n",
    "    suffix_trends_full['count'] = suffix_trends_full['count'].fillna(0)\n",
    "\n",
    "    # Plot the time series line for the suffix\n",
    "    sns.lineplot(x='data_published', y='count', data=suffix_trends_full, label=suffix)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 7 Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Suffix Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique suffixes\n",
    "# and Convert suffixes to lowercase before\n",
    "merged['suffix'] = merged['suffix'].str.lower()\n",
    "unique_suffixes = merged['suffix'].unique()\n",
    "print(unique_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group similar words using fuzzy matching\n",
    "def group_similar_words(suffixes, threshold=90):\n",
    "    groups = []\n",
    "    mapped = set()  # To keep track of words that have already been mapped\n",
    "    grouped_words = {}  # This will hold the final group mapping\n",
    "\n",
    "    while suffixes:\n",
    "        word = suffixes.pop(0)\n",
    "        if word in mapped:\n",
    "            continue  # Skip if the word is already processed\n",
    "\n",
    "        group = [word]\n",
    "        mapped.add(word)  # Mark the word as processed\n",
    "        for other_word in suffixes[:]:\n",
    "            if fuzz.ratio(word, other_word) > threshold:\n",
    "                group.append(other_word)\n",
    "                mapped.add(other_word)\n",
    "                suffixes.remove(other_word)\n",
    "        groups.append(group)\n",
    "\n",
    "    # Creating a dictionary to map similar words to the root form\n",
    "    for group in groups:\n",
    "        root_word = group[0]  # Take the first word as the root\n",
    "        for word in group:\n",
    "            grouped_words[word] = root_word\n",
    "\n",
    "    return grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to group the suffixes\n",
    "unique_suffixes = merged['suffix'].unique().tolist()\n",
    "unique_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to group the suffixes\n",
    "grouped_words = group_similar_words(unique_suffixes)\n",
    "grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Replace suffixes in the dataset with their corresponding root word\n",
    "merged['grouped_suffix'] = merged['suffix'].map(grouped_words).fillna(merged['suffix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the replacement mapping\n",
    "print(\"Suffix mapping:\")\n",
    "print(merged[['suffix', 'grouped_suffix']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Group by 'grouped_suffix' and 'data_published', sum the counts\n",
    "aggregated_data = merged.groupby(['data_published', 'grouped_suffix'])['klima_mentions_count'].sum().reset_index()\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get the top 14 most common grouped suffixes\n",
    "top_suffixes = aggregated_data[aggregated_data['grouped_suffix'] != ''] \\\n",
    "    .groupby('grouped_suffix')['klima_mentions_count'].sum().nlargest(14).index\n",
    "top_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter the data to include only the top 14 suffixes\n",
    "filtered_data = aggregated_data[aggregated_data['grouped_suffix'].isin(top_suffixes)]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data_published is treated as a datetime object\n",
    "filtered_data.loc[:, 'data_published'] = pd.to_datetime(filtered_data['data_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the trends of the top 14 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=filtered_data, x='data_published', y='klima_mentions_count', hue='grouped_suffix')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 14 Grouped Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wörter im namen mit top suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
