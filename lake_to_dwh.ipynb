{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Data Lake to Data Warehouse\n",
    "\n",
    "### What the Notebook Does\n",
    "\n",
    "- **Data Loading and Parsing:**  \n",
    "  The notebook reads HTML files containing newspaper articles along with their corresponding metadata from a CSV file. It then uses BeautifulSoup to parse the HTML and extract only the relevant content needed for further analysis.\n",
    "\n",
    "- **Data Processing and Preparation:**  \n",
    "  The extracted content is processed to isolate the contexts in which the term \"klima\" appears. This includes capturing the surrounding text to better understand the usage and meaning of the word in each article.\n",
    "\n",
    "- **Data Storage:**  \n",
    "  The processed data is structured and stored in a SQLite database with two tables. This ensures that the data is organized, easily accessible, and ready for further analysis. It will also export the data as csv for an easy import to other programms.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "- **Table: newspaper**  \n",
    "  Stores metadata about each newspaper's main page, including the publication details corresponding to a single day.  \n",
    "  Each entry represents the main page of a newspaper for one day, as the dataset is derived from crawling the main page rather than individual articles. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `newspaper_name`  \n",
    "  - `data_published`  \n",
    "  - `klima_mentions_count`\n",
    "\n",
    "- **Table: context**  \n",
    "  Contains detailed text snippets surrounding the target word \"klima\". The id refers to a newspaper (main page) from one specific day. \n",
    "  **Columns:**  \n",
    "  - `newspaper_id`  \n",
    "  - `pre_context`  \n",
    "  - `post_context`  \n",
    "  - `prefix`  \n",
    "  - `suffix`\n",
    "\n",
    "### Why This Approach\n",
    "\n",
    "- **Focused Analysis:**  \n",
    "  By isolating the contexts where \"klima\" is mentioned, the notebook prepares data specifically tailored to analyze the evolution of the term's usage over time.\n",
    "\n",
    "- **Data Organization:**  \n",
    "  Storing data in a structured SQLite database facilitates efficient querying and analysis, ensuring that subsequent analytical processes can be performed seamlessly.\n",
    "\n",
    "- **Reproducibility and Scalability:**  \n",
    "  This clear separation of tasks—from data extraction to storage—supports a reproducible workflow that can easily be extended or modified for future analytical targets.\n",
    "\n",
    "For additional details and background, please refer to the README file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "sys.path.append(os.path.abspath(\"pylib\"))\n",
    "\n",
    "import pandas as pd\n",
    "from handle_sqlite import read_table_as_dataframe\n",
    "from handle_data_processing import batch_process_newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the newspapers\n",
    "Here we will load the csv files that contain details for the newspaper like path, date and status code. For every day there is one such file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of total days: 1401\n"
     ]
    }
   ],
   "source": [
    "# Use glob to list all CSV files in the specified directory with date format in their names\n",
    "csv_files = glob.glob('data_input/data-lake/*-*.csv')\n",
    "\n",
    "# We sort here, so later we can see from startdate how the progress is till enddate\n",
    "# we sort by the filename which contains the date, ignoring the directory path to make the sort efficient\n",
    "csv_files.sort(key=lambda f: f.split('/')[-1])\n",
    "\n",
    "print(f'Count of total days: {len(csv_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read the csv files one by one to get the html file paths, only including the one with status 200 (ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store newspaper data\n",
    "newspapers = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Load CSV\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row['status'] == '200':\n",
    "                newspapers.append({\n",
    "                    'name': row['name'],\n",
    "                    'date': row['date'],\n",
    "                    'file_name': row['file_name'],\n",
    "                    'encoding': row['encoding']\n",
    "                })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newspaper in newspapers:\n",
    "    print(newspaper)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process_newspapers(newspapers, batch_size=512, num_workers=12, db_path=\"data_output/dwh_data.db\", input_path_prefix=\"data_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_data = read_table_as_dataframe(\"newspapers\", \"data_output/dwh_data.db\")\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = read_table_as_dataframe(\"context\", \"data_output/dwh_data.db\")\n",
    "context_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.to_csv(\"dwh_meta_{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data.to_csv(\"dwh_context_{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.newspaper_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.data_published.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wortwandel--z0Crv6c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
