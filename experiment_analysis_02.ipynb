{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse klima word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# make helper methods available\n",
    "sys.path.append(os.path.abspath(\"pylib\"))\n",
    "\n",
    "from handle_sqlite import read_table_as_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Seaborn settings\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from dhw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from dwh the newspaper informations per date and their usage of klima\n",
    "metadata = read_table_as_dataframe(\"newspapers\", \"data_output/dwh_data.db\")\n",
    "display(metadata.head(3))\n",
    "\n",
    "# load from dwh the found klima words\n",
    "context = read_table_as_dataframe(\"context\", \"data_output/dwh_data.db\")\n",
    "display(context.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cast types can help analysis and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = context.astype({'pre_context': 'string',\n",
    "                         'post_context': 'string',\n",
    "                         'prefix': 'string',\n",
    "                         'suffix': 'string',})\n",
    "context.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging for deeper eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again data sanity\n",
    "context[~context['newspaper_id'].isin(metadata['newspaper_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes on 'newspaper_id'\n",
    "merged = pd.merge(context, metadata, on=\"newspaper_id\", how=\"inner\")\n",
    "\n",
    "# Checking the number of unique newspaper_id in both tables\n",
    "print(f\"Unique newspaper_id in metadata: {metadata['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in context: {context['newspaper_id'].nunique()}\")\n",
    "print(f\"Unique newspaper_id in merged: {merged['newspaper_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime for proper time-based aggregation\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add maybe klima relevant dates and other big main events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined global and German-specific climate events (post 2021-04-01)\n",
    "events = {\n",
    "    'Urteil des Bundesverfassungsgerichts zum Klimaschutz': '2021-04-29',     #+ Federal Court ruling mandating stronger climate measures\n",
    "    'Bundestag beschließt neues Bundes-Klimaschutzgesetz (KSG)':'2021-06-24',\n",
    "    'IPCC AR6 Bericht (WG1)': '2021-08-06',                          #+ Release of the first working group report of AR6\n",
    "    'Deutsche Bundestagswahl': '2021-09-26',                        #+ Election impacting national climate policy direction\n",
    "    'COP26 Glasgow & Global Methane Pledge': '2021-11-01',             #+ COP26 summit held in Glasgow\n",
    "    'Beginn Ukraine-Russland-Krieg': '2022-02-24',            #+ Major geopolitical event affecting global energy and climate debate\n",
    "    'G7-Gipfel in Schloss Elmau': '2022-06-26',             #+ Key domestic conference on climate strategy and policy\n",
    "    'COP27 (UN-Klimagipfel)': '2022-11-06',             #+ COP27 summit held in Sharm El-Sheikh\n",
    "    'IPCC AR6 Synthesebericht': '2023-03-20',             #+ Comprehensive synthesis of the AR6 findings\n",
    "    'Änderung Bundes-Klimaschutzgesetz': '2023-10-04',          # Update to national climate legislation following legal mandates\n",
    "    'COP28 (UN-Klimagipfel)': '2023-11-30',             #+ UN Climate Change Conference - United Arab Emirates\n",
    "    'COP29 (UN-Klimagipfel)': '2024-11-11',             #+ UN Climate Change Conference Baku - November 2024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-step for Sentiment Analysis: Define a helper function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to the 'pre_context' and 'post_context' columns\n",
    "merged['sentiment_pre'] = merged['pre_context'].apply(get_sentiment)\n",
    "merged['sentiment_post'] = merged['post_context'].apply(get_sentiment)\n",
    "# For simplicity, combine the two sentiments (you may choose to refine this later)\n",
    "merged['sentiment'] = merged['sentiment_pre'] + ' ' + merged['sentiment_post']\n",
    "\n",
    "# Create a month-year period column for monthly aggregations\n",
    "merged['month_year'] = merged['data_published'].dt.to_period('M').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['sentiment_pre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['sentiment_post'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Trends in Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'data_published' is a datetime object\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Aggregate by month, summing the 'klima_mentions_count'\n",
    "monthly_trends = (\n",
    "    merged.groupby(merged['data_published'].dt.to_period('M'))['klima_mentions_count']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert period to string for plotting\n",
    "monthly_trends['data_published'] = monthly_trends['data_published'].astype(str)\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=monthly_trends, color='steelblue')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time (Monthly Aggregation)')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Sort by 'data_published' to ensure dates are ordered correctly\n",
    "trends_in_frequency_sorted = trends_in_frequency.sort_values('data_published')\n",
    "\n",
    "# Plot the data with dates on the x-axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_sorted, color='steelblue')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=merged['data_published'].min(), end=merged['data_published'].max(), freq='D')\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Merge the trends with the complete date range, filling missing dates with 0\n",
    "trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "# Plot the data with dates on the x-axis, treating them as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, color='steelblue')\n",
    "\n",
    "# Format the x-axis to show dates correctly, adjusting tick spacing\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=merged['data_published'].min(), end=merged['data_published'].max(), freq='D')\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Merge the trends with the complete date range, filling missing dates with 0\n",
    "trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "\n",
    "# Extract month and year from 'data_published'\n",
    "trends_in_frequency_full['month_year'] = trends_in_frequency_full['data_published'].dt.to_period('M')\n",
    "\n",
    "# Group by 'month_year' and sum 'klima_mentions_count'\n",
    "monthly_trends = trends_in_frequency_full.groupby('month_year')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Plot the data with months on the x-axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='month_year', y='klima_mentions_count', data=monthly_trends, color='steelblue')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in Frequency of the Term \"Klima\" by Month')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'newspaper_name' and sum the 'klima_mentions_count'\n",
    "media_comparison = merged.groupby('newspaper_name')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Sort by 'klima_mentions_count' to see the newspapers with the most mentions first\n",
    "media_comparison_sorted = media_comparison.sort_values('klima_mentions_count', ascending=False)\n",
    "\n",
    "# Plot the data using a horizontal bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='klima_mentions_count', y='newspaper_name', data=media_comparison_sorted, color='steelblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Total Klima Mentions')\n",
    "plt.ylabel('Newspaper Name')\n",
    "plt.title('Comparison of Klima Mentions Across Different Newspapers')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key external events (dates)\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Group by 'data_published' and sum the 'klima_mentions_count'\n",
    "trends_in_frequency = merged.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency, color='steelblue')\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions and Correlation with External Events')\n",
    "plt.legend(title=\"Key Events\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Sum 'klima_mentions_count' by newspaper\n",
    "top_newspapers = merged.groupby('newspaper_name')['klima_mentions_count'].sum().nlargest(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 newspapers\n",
    "top_newspapers_data = merged[merged['newspaper_name'].isin(top_newspapers)]\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_newspapers_data['data_published'].min(), end=top_newspapers_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each top newspaper\n",
    "plt.figure(figsize=(14, 8))\n",
    "for newspaper in top_newspapers:\n",
    "    newspaper_data = top_newspapers_data[top_newspapers_data['newspaper_name'] == newspaper]\n",
    "    \n",
    "    # Group by date and sum 'klima_mentions_count'\n",
    "    trends_in_frequency = newspaper_data.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "    \n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "    trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "    \n",
    "    # Plot the time series line for the newspaper\n",
    "    sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, label=newspaper)\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions by Newspaper and Correlation with External Events')\n",
    "plt.legend(title=\"Key Events and Newspapers\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Sum 'klima_mentions_count' by newspaper\n",
    "top_newspapers = merged.groupby('newspaper_name')['klima_mentions_count'].sum().nlargest(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 newspapers\n",
    "top_newspapers_data = merged[merged['newspaper_name'].isin(top_newspapers)]\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_newspapers_data['data_published'].min(), end=top_newspapers_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each top newspaper\n",
    "plt.figure(figsize=(14, 8))\n",
    "for newspaper in top_newspapers:\n",
    "    newspaper_data = top_newspapers_data[top_newspapers_data['newspaper_name'] == newspaper]\n",
    "    \n",
    "    # Group by date and sum 'klima_mentions_count'\n",
    "    trends_in_frequency = newspaper_data.groupby('data_published')['klima_mentions_count'].sum().reset_index()\n",
    "    \n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    trends_in_frequency_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    trends_in_frequency_full = trends_in_frequency_full.merge(trends_in_frequency, on='data_published', how='left')\n",
    "    trends_in_frequency_full['klima_mentions_count'] = trends_in_frequency_full['klima_mentions_count'].fillna(0)\n",
    "    \n",
    "    # Plot the time series line for the newspaper\n",
    "    sns.lineplot(x='data_published', y='klima_mentions_count', data=trends_in_frequency_full, label=newspaper, markers=True, marker='o')\n",
    "\n",
    "# Mark the external events with vertical lines\n",
    "for event, date in events.items():\n",
    "    plt.axvline(pd.to_datetime(date), linestyle='--', color='red', label=event)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Adjust date spacing\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Trends in \"Klima\" Mentions by Newspaper and Correlation with External Events')\n",
    "plt.legend(title=\"Key Events and Newspapers\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment analyese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Perform sentiment analysis on the 'pre_context' and 'post_context' columns\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to both 'pre_context' and 'post_context'\n",
    "merged['sentiment_pre'] = merged['pre_context'].apply(get_sentiment)\n",
    "merged['sentiment_post'] = merged['post_context'].apply(get_sentiment)\n",
    "\n",
    "# Combine both sentiments into a single column\n",
    "merged['sentiment'] = merged['sentiment_pre'] + ' ' + merged['sentiment_post']\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "# Create a date range (monthly, for example)\n",
    "merged['month_year'] = merged['data_published'].dt.to_period('M')\n",
    "\n",
    "# Count the frequency of sentiments and sum 'klima_mentions_count' for each period\n",
    "sentiment_data = merged.groupby(['month_year', 'sentiment'])['klima_mentions_count'].sum().reset_index()\n",
    "\n",
    "# Pivot the data to get separate columns for each sentiment type (positive, negative, neutral)\n",
    "sentiment_pivot = sentiment_data.pivot_table(index='month_year', columns='sentiment', values='klima_mentions_count', aggfunc='sum').fillna(0)\n",
    "\n",
    "# Plot the sentiment distribution over time (stacked bar plot)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sentiment_pivot.plot(kind='bar', stacked=True, figsize=(14, 8), color=['lightgreen', 'lightcoral', 'lightgrey'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Klima Mentions')\n",
    "plt.title('Sentiment Distribution of \"Klima\" Mentions Over Time')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'suffix' and count occurrences\n",
    "suffix_counts = merged['suffix'].value_counts().head(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 suffixes\n",
    "top_suffix_data = merged[merged['suffix'].isin(suffix_counts)]\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "top_suffix_data['data_published'] = pd.to_datetime(top_suffix_data['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_suffix_data['data_published'].min(), end=top_suffix_data['data_published'].max(), freq='M')\n",
    "\n",
    "# Plot for each of the top 7 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "for suffix in suffix_counts:\n",
    "    # Filter the data for the current suffix\n",
    "    suffix_data = top_suffix_data[top_suffix_data['suffix'] == suffix]\n",
    "    \n",
    "    # Group by date and count the occurrences of the suffix\n",
    "    suffix_trends = suffix_data.groupby('data_published').size().reset_index(name='count')\n",
    "    \n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    suffix_trends_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    suffix_trends_full = suffix_trends_full.merge(suffix_trends, on='data_published', how='left')\n",
    "    suffix_trends_full['count'] = suffix_trends_full['count'].fillna(0)\n",
    "    \n",
    "    # Plot the time series line for the suffix\n",
    "    sns.lineplot(x='data_published', y='count', data=suffix_trends_full, label=suffix)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 7 Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Suffix Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'suffix' and count occurrences\n",
    "suffix_counts = merged['suffix'].value_counts().head(7).index\n",
    "\n",
    "# Filter the data to include only the top 7 suffixes\n",
    "top_suffix_data = merged[merged['suffix'].isin(suffix_counts)]\n",
    "\n",
    "# Convert 'data_published' to datetime format\n",
    "top_suffix_data['data_published'] = pd.to_datetime(top_suffix_data['data_published'])\n",
    "\n",
    "# Create a complete date range from the min to max date in the data\n",
    "date_range = pd.date_range(start=top_suffix_data['data_published'].min(), end=top_suffix_data['data_published'].max(), freq='D')\n",
    "\n",
    "# Plot for each of the top 7 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "for suffix in suffix_counts:\n",
    "    # Filter the data for the current suffix\n",
    "    suffix_data = top_suffix_data[top_suffix_data['suffix'] == suffix]\n",
    "    \n",
    "    # Group by date and count the occurrences of the suffix\n",
    "    suffix_trends = suffix_data.groupby('data_published').size().reset_index(name='count')\n",
    "    \n",
    "    # Merge with the complete date range, filling missing dates with 0\n",
    "    suffix_trends_full = pd.DataFrame(date_range, columns=['data_published'])\n",
    "    suffix_trends_full = suffix_trends_full.merge(suffix_trends, on='data_published', how='left')\n",
    "    suffix_trends_full['count'] = suffix_trends_full['count'].fillna(0)\n",
    "    \n",
    "    # Plot the time series line for the suffix\n",
    "    sns.lineplot(x='data_published', y='count', data=suffix_trends_full, label=suffix)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 7 Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Suffix Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique suffixes\n",
    "# and Convert suffixes to lowercase before\n",
    "merged['suffix'] = merged['suffix'].str.lower()\n",
    "unique_suffixes = merged['suffix'].unique()\n",
    "print(unique_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group similar words using fuzzy matching\n",
    "def group_similar_words(suffixes, threshold=80):\n",
    "    groups = []\n",
    "    mapped = set()  # To keep track of words that have already been mapped\n",
    "    grouped_words = {}  # This will hold the final group mapping\n",
    "    \n",
    "    while suffixes:\n",
    "        word = suffixes.pop(0)\n",
    "        if word in mapped:\n",
    "            continue  # Skip if the word is already processed\n",
    "\n",
    "        group = [word]\n",
    "        mapped.add(word)  # Mark the word as processed\n",
    "        for other_word in suffixes[:]:\n",
    "            if fuzz.ratio(word, other_word) > threshold:\n",
    "                group.append(other_word)\n",
    "                mapped.add(other_word)\n",
    "                suffixes.remove(other_word)\n",
    "        groups.append(group)\n",
    "\n",
    "    # Creating a dictionary to map similar words to the root form\n",
    "    for group in groups:\n",
    "        root_word = group[0]  # Take the first word as the root\n",
    "        for word in group:\n",
    "            grouped_words[word] = root_word\n",
    "    \n",
    "    return grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to group the suffixes\n",
    "unique_suffixes = merged['suffix'].unique().tolist()\n",
    "unique_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to group the suffixes\n",
    "grouped_words = group_similar_words(unique_suffixes)\n",
    "grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Replace suffixes in the dataset with their corresponding root word\n",
    "merged['grouped_suffix'] = merged['suffix'].map(grouped_words).fillna(merged['suffix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the replacement mapping\n",
    "print(\"Suffix mapping:\")\n",
    "print(merged[['suffix', 'grouped_suffix']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Group by 'grouped_suffix' and 'data_published', sum the counts\n",
    "aggregated_data = merged.groupby(['data_published', 'grouped_suffix'])['klima_mentions_count'].sum().reset_index()\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get the top 14 most common grouped suffixes\n",
    "top_suffixes = aggregated_data[aggregated_data['grouped_suffix'] != ''] \\\n",
    "    .groupby('grouped_suffix')['klima_mentions_count'].sum().nlargest(14).index\n",
    "top_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter the data to include only the top 14 suffixes\n",
    "filtered_data = aggregated_data[aggregated_data['grouped_suffix'].isin(top_suffixes)]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data_published is treated as a datetime object\n",
    "filtered_data.loc[:, 'data_published'] = pd.to_datetime(filtered_data['data_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the trends of the top 14 suffixes\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=filtered_data, x='data_published', y='klima_mentions_count', hue='grouped_suffix')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Date Published')\n",
    "plt.ylabel('Frequency of Suffix')\n",
    "plt.title('Trends of the Top 14 Grouped Suffix Words Over Time')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wörter im namen mit top suffix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wortwandel--z0Crv6c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
