{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Datenqualität & Nullen-Analyse\n",
    "\n",
    "Dieses Notebook analysiert die **Datenqualität ab 21.04.2022** und identifiziert Zeitungen mit guter Coverage (wenig Lücken).\n",
    "\n",
    "**Ziel**: Entscheidungsgrundlage für Zeitungs-Filterung basierend auf Datenvollständigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup & Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../pylib\")\n",
    "from handle_sqlite import read_table_as_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEUE ZELLE: Datenstruktur-Check\n",
    "\n",
    "# 1. Wie viele Einträge in metadata haben klima_mentions_count = 0?\n",
    "zero_klima = metadata[metadata['klima_mentions_count'] == 0]\n",
    "print(f\"Einträge mit klima_mentions_count = 0: {len(zero_klima)}\")\n",
    "print(f\"Einträge mit klima_mentions_count > 0: {len(metadata) - len(zero_klima)}\")\n",
    "\n",
    "# 2. Beispiel: Eine Zeitung über Zeit\n",
    "example_newspaper = metadata['newspaper_name'].value_counts().index[0]\n",
    "example_df = metadata[metadata['newspaper_name'] == example_newspaper].sort_values('data_published')\n",
    "\n",
    "print(f\"\\nBeispiel: {example_newspaper}\")\n",
    "print(f\"Gesamteinträge: {len(example_df)}\")\n",
    "print(f\"Davon mit klima_mentions_count = 0: {len(example_df[example_df['klima_mentions_count'] == 0])}\")\n",
    "print(f\"Davon mit klima_mentions_count > 0: {len(example_df[example_df['klima_mentions_count'] > 0])}\")\n",
    "\n",
    "# 3. Zeitraum dieser Zeitung\n",
    "print(f\"\\nZeitraum: {example_df['data_published'].min()} bis {example_df['data_published'].max()}\")\n",
    "print(f\"Erwartete Tage: {(example_df['data_published'].max() - example_df['data_published'].min()).days + 1}\")\n",
    "print(f\"Tatsächliche Einträge: {len(example_df)}\")\n",
    "\n",
    "# 4. Sind alle erwarteten Tage abgedeckt?\n",
    "all_dates = pd.date_range(start=example_df['data_published'].min(),\n",
    "                           end=example_df['data_published'].max(),\n",
    "                           freq='D')\n",
    "missing_dates = set(all_dates) - set(example_df['data_published'])\n",
    "print(f\"\\nFehlende Tage: {len(missing_dates)}\")\n",
    "\n",
    "# 5. Wenn fehlende Tage > 0: Sind das echte Lücken oder nur Tage ohne Klima?\n",
    "if len(missing_dates) > 0:\n",
    "    print(\"→ Das sind echte Lücken (Scraper-Fehler oder nicht gecrawlt)\")\n",
    "else:\n",
    "    print(\"→ Alle Tage sind in metadata, auch wenn kein Klima erwähnt wurde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### ✅ Wichtige Erkenntnis: Datenstruktur\n",
    "\n",
    "**Frage**: Enthält `metadata` nur Tage mit Klima-Erwähnungen oder alle gecrawlten Tage?\n",
    "\n",
    "**Antwort basierend auf Quellcode-Analyse**: **Fall A (alle Tage)**\n",
    "\n",
    "Der Code in `pylib/handle_data_processing.py` sagt explizit:\n",
    "```python\n",
    "# Prepare metadata, so we also know, if there is no 'klima' at all in a newspaper\n",
    "metadata = {\n",
    "    \"newspaper_name\": name,\n",
    "    \"data_published\": date,\n",
    "    \"klima_mentions_count\": len(klima_contexts),  # kann 0 sein!\n",
    "}\n",
    "```\n",
    "\n",
    "**Konsequenzen für die Datenqualität:**\n",
    "\n",
    "- ✅ **Jeder gecrawlte Tag** hat einen Eintrag in `metadata`, auch mit `klima_mentions_count = 0`\n",
    "- ✅ **Grau in Visualisierung** = wirklich \"kein Klima erwähnt\" (aber gecrawlt!)\n",
    "- ✅ **Weiß in Visualisierung** = echte Lücke (nicht gecrawlt)\n",
    "- ✅ Wir können **unterscheiden** zwischen \"kein Klima\" und \"nicht gecrawlt\"\n",
    "\n",
    "→ Die Coverage-Analyse ist **valide** und zeigt echte Datenlücken!\n",
    "\n",
    "**Bitte führen Sie die Code-Zelle oben aus**, um dies empirisch zu bestätigen. Erwartung: Es gibt viele Einträge mit `klima_mentions_count = 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Datenstruktur-Check: Nullen vs. Lücken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2: Daten laden\n",
    "db_path = \"../data_output/dwh_data.db\"\n",
    "metadata = read_table_as_dataframe(\"newspapers\", db_path)\n",
    "context = read_table_as_dataframe(\"context\", db_path)\n",
    "\n",
    "print(f\"Metadata: {len(metadata)} Einträge\")\n",
    "print(f\"Context: {len(context)} Einträge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 3: Merge für tiefere Analyse\n",
    "merged = pd.merge(context, metadata, on=\"newspaper_id\", how=\"inner\")\n",
    "merged['data_published'] = pd.to_datetime(merged['data_published'])\n",
    "\n",
    "print(f\"Merged: {len(merged)} Einträge\")\n",
    "print(f\"\\nZeitraum: {merged['data_published'].min()} bis {merged['data_published'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Coverage-Analyse pro Zeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Min/Max Datum pro Zeitung\n",
    "coverage = merged.groupby('newspaper_name')['data_published'].agg(['min', 'max', 'count']).reset_index()\n",
    "coverage.columns = ['newspaper_name', 'first_date', 'last_date', 'total_records']\n",
    "coverage = coverage.sort_values('first_date')\n",
    "\n",
    "print(f\"Anzahl Zeitungen: {len(coverage)}\")\n",
    "coverage.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 5: Welche Zeitungen sind ab 21.04.2022 dabei?\n",
    "cutoff_date = pd.to_datetime('2022-04-21')\n",
    "newspapers_after_cutoff = coverage[coverage['first_date'] <= cutoff_date]\n",
    "\n",
    "print(f\"Zeitungen mit Daten ab/vor 21.04.2022: {len(newspapers_after_cutoff)}\")\n",
    "print(f\"Zeitungen die später starten: {len(coverage) - len(newspapers_after_cutoff)}\")\n",
    "newspapers_after_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3. Lücken-Analyse ab 21.04.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 6: Erwartete Tage vs. tatsächliche Tage (ab 21.04.2022)\n",
    "date_max = merged['data_published'].max()\n",
    "expected_days = (date_max - cutoff_date).days + 1\n",
    "\n",
    "print(f\"Analysezeitraum: {cutoff_date.date()} bis {date_max.date()}\")\n",
    "print(f\"Erwartete Tage: {expected_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 7: Pro Zeitung - Wie viele Tage haben wir ab 21.04.2022?\n",
    "newspapers_after = merged[merged['data_published'] >= cutoff_date]\n",
    "\n",
    "actual_days_per_newspaper = newspapers_after.groupby('newspaper_name')['data_published'].apply(\n",
    "    lambda x: x.dt.date.nunique()\n",
    ").reset_index()\n",
    "\n",
    "actual_days_per_newspaper.columns = ['newspaper_name', 'actual_days']\n",
    "actual_days_per_newspaper['expected_days'] = expected_days\n",
    "actual_days_per_newspaper['coverage_pct'] = (actual_days_per_newspaper['actual_days'] / expected_days) * 100\n",
    "actual_days_per_newspaper['missing_days'] = expected_days - actual_days_per_newspaper['actual_days']\n",
    "actual_days_per_newspaper = actual_days_per_newspaper.sort_values('coverage_pct', ascending=False)\n",
    "\n",
    "print(f\"Coverage-Statistik für {len(actual_days_per_newspaper)} Zeitungen:\")\n",
    "actual_days_per_newspaper.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 8: Zeitungen mit > 90% Coverage\n",
    "good_coverage = actual_days_per_newspaper[actual_days_per_newspaper['coverage_pct'] > 90]\n",
    "\n",
    "print(f\"✓ Zeitungen mit > 90% Coverage ab 21.04.2022: {len(good_coverage)}\")\n",
    "print(f\"  Durchschnittliche Coverage: {good_coverage['coverage_pct'].mean():.1f}%\")\n",
    "good_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 9: Zeitungen mit 70-90% Coverage (mittelmäßig)\n",
    "medium_coverage = actual_days_per_newspaper[\n",
    "    (actual_days_per_newspaper['coverage_pct'] >= 70) &\n",
    "    (actual_days_per_newspaper['coverage_pct'] <= 90)\n",
    "]\n",
    "\n",
    "print(f\"~ Zeitungen mit 70-90% Coverage: {len(medium_coverage)}\")\n",
    "print(f\"  Durchschnittliche Coverage: {medium_coverage['coverage_pct'].mean():.1f}%\")\n",
    "medium_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 10: Zeitungen mit < 70% Coverage (problematisch)\n",
    "bad_coverage = actual_days_per_newspaper[actual_days_per_newspaper['coverage_pct'] < 70]\n",
    "\n",
    "print(f\"✗ Zeitungen mit < 70% Coverage: {len(bad_coverage)}\")\n",
    "print(f\"  Durchschnittliche Coverage: {bad_coverage['coverage_pct'].mean():.1f}%\")\n",
    "bad_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 4. Große Nullen-Visualisierung\n",
    "\n",
    "**Legende:**\n",
    "- **Blau**: Tag mit Klima-Komposita\n",
    "- **Grau**: Tag gecrawlt, aber keine Klima-Komposita\n",
    "- **Weiß**: Lücke (kein Crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 11: Vorbereitung für Visualisierung\n",
    "metadata['data_published'] = pd.to_datetime(metadata['data_published'])\n",
    "metadata['has_klima'] = metadata['klima_mentions_count'] > 0\n",
    "\n",
    "# Nur Zeitungen die ab 21.04.2022 dabei sind\n",
    "newspapers_to_plot = actual_days_per_newspaper['newspaper_name'].tolist()\n",
    "\n",
    "# Sortiere nach Coverage (beste zuerst)\n",
    "newspapers_sorted = actual_days_per_newspaper.sort_values('coverage_pct', ascending=True)['newspaper_name'].tolist()\n",
    "\n",
    "print(f\"Visualisiere {len(newspapers_sorted)} Zeitungen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 12: Große Visualisierung - Zeitungen über Zeit mit Lücken\n",
    "date_min = cutoff_date\n",
    "date_max = metadata['data_published'].max()\n",
    "all_dates = pd.date_range(start=date_min, end=date_max, freq='D')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "y_pos = 0\n",
    "\n",
    "for np_name in newspapers_sorted:\n",
    "    np_meta = metadata[metadata['newspaper_name'] == np_name]\n",
    "    np_meta = np_meta[np_meta['data_published'] >= date_min]\n",
    "\n",
    "    crawled_dates = set(np_meta['data_published'].dt.date)\n",
    "    klima_dates = set(np_meta[np_meta['has_klima']]['data_published'].dt.date)\n",
    "\n",
    "    for d in all_dates:\n",
    "        if d.date() in klima_dates:\n",
    "            ax.barh(y_pos, 1, left=d, height=0.8, color='steelblue', edgecolor='none')\n",
    "        elif d.date() in crawled_dates:\n",
    "            ax.barh(y_pos, 1, left=d, height=0.8, color='lightgray', edgecolor='none')\n",
    "        # Weiß = Lücke (nichts plotten)\n",
    "\n",
    "    y_pos += 1\n",
    "\n",
    "ax.set_yticks(range(len(newspapers_sorted)))\n",
    "ax.set_yticklabels(newspapers_sorted, fontsize=7)\n",
    "ax.set_xlabel('Datum', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Zeitung (sortiert nach Coverage)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Datenqualität ab 21.04.2022: Blau=Klima, Grau=Ohne Klima, Weiß=Lücken',\n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.set_xlim(date_min, date_max)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 5. Erkenntnisse - Datenqualität ab 21.04.2022\n",
    "\n",
    "### Coverage-Statistik\n",
    "\n",
    "**Analysezeitraum**: 21.04.2022 bis heute  \n",
    "**Erwartete Tage**: [wird aus Daten berechnet]\n",
    "\n",
    "**Zeitungen nach Coverage:**\n",
    "- **> 90% Coverage**: [N Zeitungen] → Konsistente Datenerfassung, empfohlen für Analyse\n",
    "- **70-90% Coverage**: [M Zeitungen] → Mittlere Qualität, ggf. verwendbar mit Vorsicht\n",
    "- **< 70% Coverage**: [K Zeitungen] → Viele Lücken, Ausschluss empfohlen\n",
    "\n",
    "### Beobachtungen aus Visualisierung\n",
    "\n",
    "Die Visualisierung zeigt:\n",
    "- Welche Zeitungen durchgehend erfasst wurden (wenig weiße Lücken)\n",
    "- Welche Zeitungen sporadische Ausfälle haben\n",
    "- Welche Zeitungen sehr unvollständig sind\n",
    "\n",
    "### Empfehlung für nächste Schritte\n",
    "\n",
    "1. **Zeitungs-Filter**: Nur Zeitungen mit > 90% Coverage verwenden\n",
    "2. **Sprachfilter**: Zusätzlich nur deutsche Zeitungen (keine englischen wie BBC, CNN, etc.)\n",
    "3. **Suffix-Analyse**: Häufigste deutsche Klima-Komposita identifizieren\n",
    "4. **Zeitfilter**: Ab 21.04.2022 für konsistente Datenbasis\n",
    "\n",
    "### Nächstes Notebook\n",
    "\n",
    "**03_Suffix_Analyse.ipynb**: Welche deutschen Klima-Komposita sind am häufigsten?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wortwandel--z0Crv6c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
